{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubiCasa / FloorplantoBlenderlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1gRB7ez1e4H7a9Y09lLqRuna0luZO5VRK\n",
      "From (redirected): https://drive.google.com/uc?id=1gRB7ez1e4H7a9Y09lLqRuna0luZO5VRK&confirm=t&uuid=80657394-a58b-4731-8abf-4992f1278a6d\n",
      "To: /Users/luq/dev/projects/cv-floorplan/rbg-research_Floor-Plan-Detection-main/model_best_val_loss_var.pkl\n",
      "100%|████████████████████████████████████████| 209M/209M [01:11<00:00, 2.92MB/s]\n",
      "mkdir: missing operand\n",
      "Try 'mkdir --help' for more information.\n",
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open '-C'\n",
      "rm: cannot remove 'blender-2.93.1-linux-x64.tar.xz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#downloading weights \n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1gRB7ez1e4H7a9Y09lLqRuna0luZO5VRK'\n",
    "\n",
    "#downloading blender\n",
    "\n",
    "blender_url =  \"https://ftp.nluug.nl/pub/graphics/blender/release/Blender2.93/blender-2.93.1-linux-x64.tar.xz\"\n",
    "base_url = os.path.basename(blender_url)\n",
    "!mkdir $blender_version\n",
    "!wget -nc $blender_url\n",
    "!tar -xkf $base_url -C ./$blender_version --strip-components=1\n",
    "!rm blender-2.93.1-linux-x64.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add outer folder \n",
    " # Adds higher directory to python modules path.\n",
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "\n",
    "# Other necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Image\n",
    "Now we need an example image to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input image: ![input](Images/example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Contours (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x468>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "\n",
    "import cv2 # for image gathering\n",
    "import numpy as np\n",
    "\n",
    "# for visualize\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img_path = \"Images/test_floorplan2_resized512.png\"\n",
    "\n",
    "# Read floorplan image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Create blank image\n",
    "height, width, channels = img.shape\n",
    "blank_image = np.zeros((height,width,3), np.uint8)\n",
    "\n",
    "# Grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect outer Contours (simple floor or roof solution), paint them red on blank_image\n",
    "contour, img = detect.detectOuterContours(gray, blank_image, color=(255,0,0))\n",
    "\n",
    "# Display\n",
    "display(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Rooms (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAHUCAIAAADDXQcJAAACz0lEQVR4nO3BMQEAAADCoPVP7WkJoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABu+moAAe8u8mgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x468>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(img_path)\n",
    "\n",
    "    # grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# create verts (points 3d), points to use in mesh creations\n",
    "verts = []\n",
    "# create faces for each plane, describe order to create mesh points\n",
    "faces = []\n",
    "\n",
    "# Height of waLL\n",
    "height = 0.999\n",
    "\n",
    "# Scale pixel value to 3d pos\n",
    "scale = 100\n",
    "\n",
    "gray = detect.wall_filter(gray)\n",
    "\n",
    "gray = ~gray\n",
    "\n",
    "rooms, colored_rooms = detect.find_rooms(gray.copy())\n",
    "\n",
    "gray_rooms =  cv2.cvtColor(colored_rooms,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# get box positions for rooms\n",
    "boxes, gray_rooms = detect.detectPreciseBoxes(gray_rooms, gray_rooms)\n",
    "display(Image.fromarray(colored_rooms))\n",
    "\n",
    " #Create verts\n",
    "room_count = 0\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, height)])\n",
    "    room_count+= 1\n",
    "\n",
    "# create faces\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for pos in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Walls (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Generate wall data file for floorplan\n",
    "    @Param img_path, path to input file\n",
    "    @Param info, boolean if data should be printed\n",
    "    @Return shape\n",
    "    '''\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "# Read floorplan image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# create wall image (filter out small objects from image)\n",
    "wall_img = detect.wall_filter(gray)\n",
    "\n",
    "# detect walls\n",
    "boxes, img = detect.detectPreciseBoxes(wall_img)\n",
    "\n",
    "display(Image.fromarray(wall_img))\n",
    "\n",
    "# create verts (points 3d), points to use in mesh creations\n",
    "verts = []\n",
    "# create faces for each plane, describe order to create mesh points\n",
    "faces = []\n",
    "\n",
    "# Height of waLL\n",
    "wall_height = 1\n",
    "\n",
    "# Scale pixel value to 3d pos\n",
    "scale = 100\n",
    "\n",
    "# Convert boxes to verts and faces\n",
    "verts, faces, wall_amount = transform.create_nx4_verts_and_faces(boxes, wall_height, scale)\n",
    "\n",
    "# Create top walls verts\n",
    "verts = []\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, 0)])\n",
    "\n",
    "# create faces\n",
    "faces = []\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for _ in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubiCasa Pre-Trained Object/Room Segmentation          \n",
    "# (Deep Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m n_icons \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m#Check if shape of image is odd or even\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     size_check \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m])\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     57\u001b[0m     height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m size_check[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m     width \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m size_check[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2 \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import get_model\n",
    "from utils.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns\n",
    "from utils.plotting import segmentation_plot, polygons_to_image, draw_junction_from_dict, discrete_cmap\n",
    "#discrete_cmap()\n",
    "from utils.post_prosessing import split_prediction, get_polygons, split_validation\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "rot = RotateNTurns()\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\",\n",
    "                \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "icon_classes = [\"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\",\n",
    "                \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\"]\n",
    "\n",
    "model = get_model('hg_furukawa_original', 51)\n",
    "n_classes = 44\n",
    "split = [21, 12, 11]\n",
    "model.conv4_ = torch.nn.Conv2d(256, n_classes, bias=True, kernel_size=1)\n",
    "model.upsample = torch.nn.ConvTranspose2d(n_classes, n_classes, kernel_size=4, stride=4)\n",
    "checkpoint = torch.load('model_best_val_loss_var.pkl', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# Create tensor for pytorch\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # correct color channels\n",
    "\n",
    "# Image transformation to range (-1,1)\n",
    "img = 2 * (img / 255.0) - 1\n",
    "\n",
    "# Move from (h,w,3)--->(3,h,w) as model input dimension is defined like this\n",
    "img = np.moveaxis(img, -1, 0)\n",
    "\n",
    "# Convert to pytorch, enable cuda\n",
    "img = torch.tensor([img.astype(np.float32)]).cuda()\n",
    "n_rooms = 12\n",
    "n_icons = 11\n",
    "\n",
    "with torch.no_grad():\n",
    "    #Check if shape of image is odd or even\n",
    "    size_check = np.array([img.shape[2],img.shape[3]])%2\n",
    "\n",
    "    height = img.shape[2] - size_check[0]\n",
    "    width = img.shape[3] - size_check[1]\n",
    "\n",
    "    img_size = (height, width)\n",
    "\n",
    "    rotations = [(0, 0), (1, -1), (2, 2), (-1, 1)]\n",
    "    pred_count = len(rotations)\n",
    "    prediction = torch.zeros([pred_count, n_classes, height, width])\n",
    "    for i, r in enumerate(rotations):\n",
    "        forward, back = r\n",
    "        # We rotate first the image\n",
    "        rot_image = rot(img, 'tensor', forward)\n",
    "        pred = model(rot_image)\n",
    "        # We rotate prediction back\n",
    "        pred = rot(pred, 'tensor', back)\n",
    "        # We fix heatmaps\n",
    "        pred = rot(pred, 'points', back)\n",
    "        # We make sure the size is correct\n",
    "        pred = F.interpolate(pred, size=(height, width), mode='bilinear', align_corners=True)\n",
    "        # We add the prediction to output\n",
    "        prediction[i] = pred[0]\n",
    "\n",
    "prediction = torch.mean(prediction, 0, True)\n",
    "\n",
    "\n",
    "rooms_pred = F.softmax(prediction[0, 21:21+12], 0).cpu().data.numpy()\n",
    "rooms_pred = np.argmax(rooms_pred, axis=0)\n",
    "\n",
    "icons_pred = F.softmax(prediction[0, 21+12:], 0).cpu().data.numpy()\n",
    "icons_pred = np.argmax(icons_pred, axis=0)\n",
    "\n",
    "heatmaps, rooms, icons = split_prediction(prediction, img_size, split)\n",
    "polygons, types, room_polygons, room_types = get_polygons((heatmaps, rooms, icons), 0.2, [1, 2])\n",
    "\n",
    "wall_polygon_numbers=[i for i,j in enumerate(types) if j['type']=='wall']\n",
    "boxes=[]\n",
    "for i,j in enumerate(polygons):\n",
    "    if i in wall_polygon_numbers:\n",
    "        temp=[]\n",
    "        for k in j:\n",
    "            temp.append(np.array([k]))\n",
    "        boxes.append(np.array(temp))\n",
    "        \n",
    "verts, faces, wall_amount = transform.create_nx4_verts_and_faces(boxes, wall_height, scale)\n",
    "\n",
    "# Create top walls verts\n",
    "verts = []\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, 0)])\n",
    "\n",
    "# create faces\n",
    "faces = []\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for _ in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pol_room_seg, pol_icon_seg = polygons_to_image(polygons, types, room_polygons, room_types, height, width)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.axis('off')\n",
    "rseg = ax.imshow(pol_room_seg, cmap='rooms', vmin=0, vmax=n_rooms-0.1)\n",
    "cbar = plt.colorbar(rseg, ticks=np.arange(n_rooms) + 0.5, fraction=0.046, pad=0.01)\n",
    "cbar.ax.set_yticklabels(room_classes, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.axis('off')\n",
    "iseg = ax.imshow(pol_icon_seg, cmap='icons', vmin=0, vmax=n_icons-0.1)\n",
    "cbar = plt.colorbar(iseg, ticks=np.arange(n_icons) + 0.5, fraction=0.046, pad=0.01)\n",
    "cbar.ax.set_yticklabels(icon_classes, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data files and Blender files (Using CubiCasa and Super-Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/2Dto3D/GIT/ftb_CubiCasa_v4/floortrans/plotting.py:591: UserWarning: Trying to register the cmap 'rooms' which already exists.\n",
      "  cm.register_cmap(cmap=cmap3)\n",
      "/home/ubuntu/2Dto3D/GIT/ftb_CubiCasa_v4/floortrans/plotting.py:597: UserWarning: Trying to register the cmap 'icons' which already exists.\n",
      "  cm.register_cmap(cmap=cmap3)\n",
      "/home/ubuntu/2Dto3D/GIT/ftb_CubiCasa_v4/floortrans/plotting.py:605: UserWarning: Trying to register the cmap 'rooms_furu' which already exists.\n",
      "  cm.register_cmap(cmap=cmap3)\n",
      "/home/ubuntu/2Dto3D/GIT/ftb_CubiCasa_v4/floortrans/plotting.py:611: UserWarning: Trying to register the cmap 'rooms_furu' which already exists.\n",
      "  cm.register_cmap(cmap=cmap3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----- Generate  Images/example.png  at pos  None  rot  None  -----\n",
      "Approximated apartment size :  1731843.0\n",
      "Created file : Data/2/floor_verts.txt\n",
      "Created file : Data/2/floor_faces.txt\n",
      "Walls created :  172\n",
      "Created file : Data/2/wall_verts.txt\n",
      "Created file : Data/2/wall_faces.txt\n",
      "Created file : Data/2/top_wall_verts.txt\n",
      "Created file : Data/2/top_wall_faces.txt\n",
      "Number of rooms detected :  22\n",
      "Created file : Data/2/rooms_verts.txt\n",
      "Created file : Data/2/rooms_faces.txt\n",
      "Created file : Data/2/transform.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import FloorplanToSTL as stl\n",
    "import config\n",
    "\n",
    "# can specify or use default paths in config files\n",
    "stl.createFloorPlan(image_path = config.image_path, target_path = config.target_path, SR_Check=True)\n",
    "\n",
    "# Note: USE SR_Check = False for Original Image "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
